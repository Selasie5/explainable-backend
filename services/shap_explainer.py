
import shap
import pandas as pd
import matplotlib.pyplot as plt
import os
import uuid
import numpy as np
import logging
from typing import Union, Dict, List
from sklearn.base import BaseEstimator
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from starlette.responses import StreamingResponse
import io
import base64
from schema.explain_response import ExplainResponse, ModelInfo, PredictionContext, LocalExplanation, GlobalExplanation, ArtifactURLs, NaturalLanguage, WhatIf, Provenance, Contributor
from services.explain_utils import get_top_contributors, summarize_local, summarize_global
from datetime import datetime, timezone

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create explanations output directory
EXPLANATION_OUTPUT_DIR = "explanations"
os.makedirs(EXPLANATION_OUTPUT_DIR, exist_ok=True)

def is_tree_based_model(model: BaseEstimator) -> bool:
    """Check if the model is tree-based"""
    return isinstance(model, (XGBClassifier, LGBMClassifier,
                            RandomForestClassifier, GradientBoostingClassifier))

def generate_shap_explanations(model: BaseEstimator, data: Union[np.ndarray, 'pd.DataFrame']) -> Dict:
    """
    Generate SHAP explanations for a given model and dataset
    
    Args:
        model: Trained machine learning model
        data: Input data (pandas DataFrame or numpy array)
    
    Returns:
        Dictionary containing:
        - summary_plot_path: Base64 encoded image (temporary field name)
        - force_plot_path: Path to force plot HTML
        - feature_importance: Dictionary of feature importances
        - status: Success/error status
        - message: Detailed status message
    """
    try:
        # Convert data to numpy array if needed and get feature names
        X = data.values if hasattr(data, 'values') else np.array(data)
        feature_names = (data.columns.tolist() if hasattr(data, 'columns') 
                        else [f'feature_{i}' for i in range(X.shape[1])])

        # Initialize appropriate SHAP explainer
        if is_tree_based_model(model):
            explainer = shap.TreeExplainer(model)
        elif isinstance(model, (LogisticRegression, RidgeClassifier)):
            if hasattr(data, 'columns') and not hasattr(model, 'feature_names_in_'):
                logger.warning("Model fitted without feature names. Using provided feature names.")
                if hasattr(model, 'set_output'):
                    model.set_output(transform="pandas")
            masker = shap.maskers.Independent(X, max_samples=100)
            explainer = shap.LinearExplainer(model, masker)
        else:
            explainer = shap.Explainer(model, X)

        # Compute SHAP values
        shap_values = explainer(X)
        if not isinstance(shap_values, shap.Explanation):
            base_value = explainer.expected_value
            if isinstance(base_value, np.ndarray) and len(base_value) == 1:
                base_value = base_value[0]
            shap_values = shap.Explanation(
                values=shap_values.values if hasattr(shap_values, 'values') else shap_values,
                base_values=base_value,
                data=X,
                feature_names=feature_names
            )

        # --- Prediction context ---
        row_id = str(uuid.uuid4())
        preds = model.predict(X)
        probs = model.predict_proba(X)[0] if hasattr(model, "predict_proba") else None
        class_names = model.classes_ if hasattr(model, "classes_") else ["class_0", "class_1"]
        predicted_class = class_names[np.argmax(probs)] if probs is not None else str(preds[0])
        class_probabilities = {str(cls): float(prob) for cls, prob in zip(class_names, probs)} if probs is not None else {}
        decision_threshold = 0.5  # or extract from model if available
        base_value = float(shap_values.base_values[0]) if hasattr(shap_values, "base_values") else None

        # --- Local explanation ---
        top_contributors = get_top_contributors(X[0], shap_values.values[0], feature_names, predicted_class, n=3)
        sum_shap = float(np.sum(np.abs(shap_values.values[0])))

        # --- Global explanation ---
        feature_importances = compute_feature_importances(shap_values)
        global_imp = {feature: float(importance) for feature, importance in zip(feature_names, feature_importances)}
        n_samples = X.shape[0]
        global_summary = summarize_global(global_imp)

        # --- Artifacts ---
        summary_plot_url = f"/static/summary_plot_{row_id}.png"
        force_plot_html_url = f"/static/force_plot_{row_id}.html"
        dependence_plot_urls = {}  # Optionally generate and fill

        # Save summary plot
        buf = io.BytesIO()
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)
        plt.tight_layout()
        plt.savefig(os.path.join("static", f"summary_plot_{row_id}.png"), format="png", bbox_inches='tight', dpi=300)
        plt.close()

        # Save force plot
        force_plot_path = os.path.join("static", f"force_plot_{row_id}.html")
        _ = generate_force_plot(shap_values, explainer, feature_names, model, force_plot_path)

        # --- Natural language ---
        local_summary = summarize_local(class_probabilities.get(predicted_class, 0), base_value, top_contributors)

        # --- What-if (simple sensitivity) ---
        what_if = [{
            "feature": top_contributors[0]["feature"],
            "suggested_change": f"decrease to {X[0][0] - 1}",
            "estimated_effect": "probability â†“ ~0.10",
            "method": "1D sensitivity via partial perturbation",
            "disclaimer": "Estimate; not a causal claim."
        }]

        # --- Provenance ---
        provenance = {
            "data_source": "from_cache",
            "model_source": "from_cache",
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "explain_algo": "TreeSHAP"
        }

        # --- Model info ---
        model_info = {
            "type": type(model).__name__,
            "version": datetime.now().strftime("%Y-%m-%d"),
            "classes": list(class_names)
        }

        # --- Build response ---
        return ExplainResponse(
            status="success",
            message="SHAP explanations generated successfully.",
            model=ModelInfo(**model_info),
            prediction=PredictionContext(
                row_id=row_id,
                predicted_class=predicted_class,
                class_probabilities=class_probabilities,
                decision_threshold=decision_threshold,
                base_value=base_value,
                output_scale="probability"
            ),
            local_explanation=LocalExplanation(
                top_contributors=[Contributor(**c) for c in top_contributors],
                sum_shap=sum_shap
            ),
            global_explanation=GlobalExplanation(
                feature_importance_mean_abs=global_imp,
                n_samples=n_samples,
                notes="Mean |SHAP| over the explained batch."
            ),
            artifacts=ArtifactURLs(
                summary_plot_url=summary_plot_url,
                force_plot_html_url=force_plot_html_url,
                dependence_plot_urls=dependence_plot_urls
            ),
            natural_language=NaturalLanguage(
                global_summary=global_summary,
                local_summary=local_summary
            ),
            what_if=[WhatIf(**w) for w in what_if],
            provenance=Provenance(**provenance)
        ).dict()
    except Exception as e:
        logger.error(f"SHAP explanation generation failed: {str(e)}")
        return {"status": "error", "message": f"Error: {str(e)}"}

def generate_force_plot(shap_values: shap.Explanation, 
                       explainer: shap.Explainer, 
                       feature_names: List[str], 
                       model: BaseEstimator,
                       force_plot_path: str = None) -> Union[str, None]:
    """
    Generate force plot with comprehensive error handling and fallbacks
    
    Args:
        shap_values: SHAP values Explanation object
        explainer: SHAP explainer instance
        feature_names: List of feature names
        model: The trained model
    
    Returns:
        Path to saved force plot HTML or None if generation failed
    """
    force_path = force_plot_path or None
    expected_value = explainer.expected_value
    
    # Convert expected_value to proper format
    if isinstance(expected_value, np.ndarray) and len(expected_value) == 1:
        expected_value = expected_value[0]
    
    try:
        # Handle different model types and SHAP value shapes
        if len(shap_values.shape) == 3:  # Multiclass case
            predicted_class = np.argmax(shap_values.values[0].sum(axis=0))
            ev = expected_value[predicted_class] if isinstance(expected_value, np.ndarray) else expected_value
            values = shap_values.values[0, predicted_class]
        else:  # Binary or regression
            ev = expected_value[0] if isinstance(expected_value, np.ndarray) else expected_value
            values = shap_values.values[0]
        
        # Generate the force plot with proper parameter ordering
        force_plot = shap.plots.force(
            base_value=ev,
            shap_values=values,
            features=shap_values.data[0],
            feature_names=feature_names,
            matplotlib=False
        )
        
        # Save the plot
        if force_path is None:
            force_path = os.path.join(EXPLANATION_OUTPUT_DIR, f"{uuid.uuid4()}_force_plot.html")
        shap.save_html(force_path, force_plot)
        
    except Exception as e:
        logger.error(f"Primary force plot generation failed: {str(e)}")
        try:
            # Fallback 1: Try without feature names
            force_plot = shap.plots.force(
                base_value=ev,
                shap_values=values,
                matplotlib=False
            )
            force_path = os.path.join(EXPLANATION_OUTPUT_DIR, f"{uuid.uuid4()}_force_plot.html")
            shap.save_html(force_path, force_plot)
        except Exception as e:
            logger.error(f"Fallback 1 force plot generation failed: {str(e)}")
            try:
                # Fallback 2: Try simplest possible force plot
                force_plot = shap.plots.force(
                    base_value=expected_value[0] if isinstance(expected_value, np.ndarray) else expected_value,
                    shap_values=shap_values.values[0],
                    matplotlib=False
                )
                force_path = os.path.join(EXPLANATION_OUTPUT_DIR, f"{uuid.uuid4()}_force_plot.html")
                shap.save_html(force_path, force_plot)
            except Exception as e:
                logger.error(f"Fallback 2 force plot generation failed: {str(e)}")
    
    return force_path

def compute_feature_importances(shap_values: shap.Explanation) -> np.ndarray:
    """Compute feature importances from SHAP values"""
    if len(shap_values.shape) == 3:  # Multiclass
        return np.mean(np.abs(shap_values.values), axis=(0, 1))
    else:  # Binary or regression
        return np.mean(np.abs(shap_values.values), axis=0)
